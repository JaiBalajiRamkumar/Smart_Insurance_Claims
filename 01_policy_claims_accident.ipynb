{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbd9e3e-07a3-4aab-a6d0-20dbfce79b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Policy & Claims DLT Ingestion pipeline\n",
    "* Tables:\n",
    "  * bronze_claim & bronze_policy\n",
    "  * silver_claim & silver_policy\n",
    "  * silver_claim_policy (joined by policy id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056b92d6-eb06-4b8e-ae5b-a822f862ef97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.mv(\"dbfs:/tmp/smart_claims\", \"dbfs:/FileStore/smart_claims\", recurse=True)\n",
    "claims_path = \"/Volumes/workspace/default/resource/data_sources/Claims/\"\n",
    "policy_path = \"/Volumes/workspace/default/resource/data_sources/Policies/policies.csv\"\n",
    "accident_path = \"/Volumes/workspace/default/resource/data_sources/Accidents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfa8adb-11c3-403d-8c6a-3d68ad497f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import lit, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b264cf2b-1c11-462b-aa6a-ec59099da2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def flatten(df):\n",
    "    complex_fields = dict([\n",
    "        (field.name, field.dataType) \n",
    "        for field in df.schema.fields \n",
    "        if isinstance(field.dataType, T.ArrayType) or isinstance(field.dataType, T.StructType)\n",
    "    ])\n",
    "    \n",
    "    qualify = list(complex_fields.keys())[0] + \"_\"\n",
    " \n",
    "    while len(complex_fields) != 0:\n",
    "        col_name = list(complex_fields.keys())[0]\n",
    "        \n",
    "        if isinstance(complex_fields[col_name], T.StructType):\n",
    "            expanded = [F.col(col_name + '.' + k).alias(col_name + '_' + k) \n",
    "                        for k in [ n.name for n in  complex_fields[col_name]]\n",
    "                       ]\n",
    "            \n",
    "            df = df.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "        elif isinstance(complex_fields[col_name], T.ArrayType): \n",
    "            df = df.withColumn(col_name, F.explode(col_name))\n",
    "    \n",
    "      \n",
    "        complex_fields = dict([\n",
    "            (field.name, field.dataType)\n",
    "            for field in df.schema.fields\n",
    "            if isinstance(field.dataType, T.ArrayType) or isinstance(field.dataType, T.StructType)\n",
    "        ])\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbfb4c4b-99cc-4f3a-ae99-133d50d101d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  comment=\"The raw claims data loaded from json files.\"\n",
    ")\n",
    "def bronze_claim():\n",
    "  return (spark.read.json(claims_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7441e2f4-7114-420f-93e9-ede7169d3aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "def bronze_policy():\n",
    "  return spark.read.option(\"header\", \"true\") \\\n",
    "          .option(\"sep\", \",\") \\\n",
    "          .format(\"csv\") \\\n",
    "          .load(policy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93dfe723-55c3-448a-86e2-bba4989ffc6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name             = \"silver_policy\",\n",
    "    comment          = \"Curated policy records\",\n",
    "    table_properties = {\n",
    "        \"layer\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\",\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "# @dlt.expect(\"valid_sum_insured\", \"sum_insured > 0\")\n",
    "@dlt.expect_all_or_drop({\n",
    "    \"valid_sum_insured\": \"sum_insured > 0\",\n",
    "    \"valid_policy_number\": \"policy_no IS NOT NULL\",\n",
    "    \"valid_premium\": \"premium > 1\",\n",
    "    \"valid_issue_date\": \"pol_issue_date < current_date()\",\n",
    "    \"valid_effective_date\": \"pol_eff_date < current_date()\",\n",
    "    \"valid_expiry_date\": \"pol_expiry_date <= current_date()\",\n",
    "    \"valid_model_year\": \"model_year > 0\"\n",
    "})\n",
    "def silver_policy():\n",
    "    # Read the staged policy records into memory\n",
    "    staged_policy = dlt.read(\"bronze_policy\")\n",
    " \n",
    "    # Update the policy premium values\n",
    "    silver_policy = staged_policy.withColumn(\"premium\", F.abs(F.col(\"premium\"))) \\\n",
    "        .withColumn(\n",
    "            # Reformat the incident date values\n",
    "            \"pol_eff_date\", F.to_date(F.col(\"pol_eff_date\"), \"dd-MM-yyyy\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            # Reformat the incident date values\n",
    "            \"pol_expiry_date\", F.to_date(F.col(\"pol_expiry_date\"), \"dd-MM-yyyy\")\n",
    "         ) \\\n",
    "        .withColumn(\n",
    "            # Reformat the incident date values\n",
    "            \"pol_issue_date\", F.to_date(F.col(\"pol_issue_date\"), \"dd-MM-yyyy\")\n",
    "         ) \n",
    "      \n",
    "    # Return the curated dataset\n",
    "    return silver_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b004ecb-1bf4-4cea-9846-75e9ef3f0386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name             = \"silver_claim\",\n",
    "    comment          = \"Curated claim records\",\n",
    "    table_properties = {\n",
    "        \"layer\": \"silve\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\",\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    " \n",
    "@dlt.expect_all_or_drop({\n",
    "    \"valid_claim_date\": \"claim_date < current_date()\",\n",
    "    \"valid_incident_date\": \"incident_date < current_date()\",\n",
    "    \"valid_incident_hour\": \"incident_hour between 0 and 24\",\n",
    "    \"valid_driver_age\": \"driver_age > 16\",\n",
    "     \"valid_driver_license\": \"driver_license_issue_date > (current_date() - cast(cast(driver_age AS INT) AS INTERVAL YEAR))\",\n",
    "    \"valid_claim_amount\": \"claim_amount_total > 0\"\n",
    " \n",
    "})\n",
    "def silver_claim():\n",
    "    # Read the staged claim records into memory\n",
    "    staged_claim = dlt.read(\"bronze_claim\")\n",
    "    # Unpack all nested attributes to create a flattened table structure\n",
    "    curated_claim = flatten(staged_claim)    \n",
    " \n",
    "    \n",
    "    # Update the format of all date/time features\n",
    "    silver_claim = curated_claim \\\n",
    "        .withColumn(\n",
    "            # Reformat the claim date values\n",
    "            \"claim_date\", F.to_date(F.col(\"claim_date\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            # Reformat the incident date values\n",
    "            \"incident_date\", F.to_date(F.col(\"incident_date\"), \"yyyy-MM-dd\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            # Reformat the driver license issue date values\n",
    "            \"driver_license_issue_date\", F.to_date(F.col(\"driver_license_issue_date\"), \"dd-MM-yyyy\")\n",
    "        ) \n",
    " \n",
    "    # Return the curated dataset\n",
    "    return silver_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd20251-70c2-4617-b118-9d4afbf29d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name             = \"silver_claim_policy\",\n",
    "    comment          = \"Curated claim joined with policy records\",\n",
    "    table_properties = {\n",
    "        \"layer\": \"silve\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\",\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_all({\n",
    "    \"valid_claim_number\": \"claim_no IS NOT NULL\",\n",
    "    \"valid_policy_number\": \"policy_no IS NOT NULL\",\n",
    "    \"valid_effective_date\": \"pol_eff_date < current_date()\",\n",
    "    \"valid_expiry_date\": \"pol_expiry_date <= current_date()\"\n",
    "  \n",
    "})\n",
    "  \n",
    "def silver_claim_policy():\n",
    "  return (dlt.read(\"silver_claim\").join(dlt.read(\"silver_policy\"), on=\"policy_no\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_policy_claims_accident",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
